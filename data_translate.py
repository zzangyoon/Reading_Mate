import os
import json
import time
from openai import OpenAI  # ì˜ˆì‹œë¡œ OpenAI ì‚¬ìš©
# client = OpenAI(api_key="YOUR_API_KEY")

# ----------------------------------------------------
# 1. ì„¤ì • ë° ë°ì´í„° ê²½ë¡œ ì •ì˜
# ----------------------------------------------------
BOOK_FOLDER = 'little_women'
CLEANED_DIR = os.path.join('data', '02_cleaned', BOOK_FOLDER)
PROCESSED_DIR = os.path.join('data', '03_processed')
# book_meta.json íŒŒì¼ì—ì„œ ì •ë³´ ë¡œë“œ (book_id, title ë“±)
META_FILE_PATH = os.path.join('data', 'book_meta.json') 

INPUT_EN_PATH = os.path.join(CLEANED_DIR, 'en.txt')
OUTPUT_JSON_PATH = os.path.join(PROCESSED_DIR, 'chunked_data_little_women.json') 

# ----------------------------------------------------
# 2. í—¬í¼ í•¨ìˆ˜
# ----------------------------------------------------

def load_book_metadata(meta_path, book_folder):
    """book_meta.jsonì—ì„œ íŠ¹ì • ì±…ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(meta_path, 'r', encoding='utf-8') as f:
        meta_data = json.load(f)
        for book in meta_data:
            if book.get('raw_folder') == book_folder:
                return book
    raise FileNotFoundError(f"'{book_folder}'ì— í•´ë‹¹í•˜ëŠ” ì±… ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def simple_chunking(text: str) -> list:
    """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨(ë‘ ê°œ ì´ìƒì˜ ì¤„ë°”ê¿ˆ) ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    # ë¬¸ë‹¨ ë¶„í• : ë‘ ê°œ ì´ìƒì˜ ì—°ì†ëœ ì¤„ë°”ê¿ˆ('\n\n')ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ”
    chunks = text.split('\n\n')
    # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ì²­í¬ë¥¼ ì œê±°í•˜ê³ , ì–‘ìª½ ê³µë°± ì œê±°
    return [chunk.strip() for chunk in chunks if chunk.strip()]

def translate_chunk(client, text_en: str, model: str = "gpt-4o"):
    """LLM APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤."""
    # ì‹¤ì œ LLM API í˜¸ì¶œ ë¡œì§ì„ ì—¬ê¸°ì— êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
    
    # [ì£¼ì˜] ì´ ë¶€ë¶„ì€ LLM API ì‚¬ìš©ë£Œì™€ ì§ê²°ë˜ë¯€ë¡œ ë°˜ë“œì‹œ ì‹¤ì œ API í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì•ˆì „í•˜ê³  ì •í™•í•œ ë²ˆì—­ì„ ìœ„í•´ JSON ëª¨ë“œë‚˜ ì‘ë‹µ í¬ë§· ì§€ì •ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
    
    try:
        # ğŸŒŸ ì‹¤ì œ API í˜¸ì¶œ ì˜ˆì‹œ (OpenAI) ğŸŒŸ
        # response = client.chat.completions.create(
        #     model=model,
        #     messages=[
        #         {"role": "system", "content": "You are a professional Korean translator. Translate the following English text into natural, formal Korean."},
        #         {"role": "user", "content": text_en}
        #     ],
        #     temperature=0.3
        # )
        # return response.choices[0].message.content.strip()
        
        # ì„ì‹œ (Dummy) ë²ˆì—­: ì‹¤ì œ LLM API í‚¤ê°€ ì—†ì„ ë•Œ í…ŒìŠ¤íŠ¸ìš©
        return f"[ë²ˆì—­ë¨] {text_en[:50]}..." # ì˜ˆì‹œë¡œ ì• 50ìë§Œ ì‚¬ìš©

    except Exception as e:
        print(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ë²ˆì—­ ì˜¤ë¥˜"

# ----------------------------------------------------
# 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ----------------------------------------------------

def main():
    # í´ë” ìƒì„± í™•ì¸
    os.makedirs(PROCESSED_DIR, exist_ok=True)
    
    try:
        # 1. ë©”íƒ€ë°ì´í„° ë¡œë“œ
        book_meta = load_book_metadata(META_FILE_PATH, BOOK_FOLDER)
        book_id = book_meta['book_id']

        # 2. ì˜ì–´ ì •ì œ í…ìŠ¤íŠ¸ ë¡œë“œ
        with open(INPUT_EN_PATH, 'r', encoding='utf-8') as f:
            full_text_en = f.read()
            
        # 3. ì²­í¬ ë¶„í• 
        english_chunks = simple_chunking(full_text_en)
        print(f"ì´ {len(english_chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # 4. ì²­í¬ ì²˜ë¦¬ ë° ë²ˆì—­
        processed_chunks = []
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì‹¤ì œ ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ)
        # client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
        
        for i, chunk_en in enumerate(english_chunks):
            print(f"-> ì²­í¬ {i+1}/{len(english_chunks)} ë²ˆì—­ ì¤‘...")
            
            # ğŸŒŸ [ì¤‘ìš”] ì‹¤ì œ ë²ˆì—­ í•¨ìˆ˜ í˜¸ì¶œ ğŸŒŸ
            # chunk_ko = translate_chunk(client, chunk_en) 
            chunk_ko = translate_chunk(None, chunk_en) # Dummy í˜¸ì¶œ
            
            processed_chunks.append({
                "chunk_index": i,
                # ì´ ë‹¨ê³„ì—ì„œ ì±•í„° ì´ë¦„ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ë³µì¡í•œ ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜,
                # ì¼ë‹¨ì€ ìˆ˜ë™ ë˜ëŠ” ê°„ë‹¨í•œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì²˜ë¦¬ ê°€ì •
                "chapter_name": f"Chapter {i//10 + 1}", # ì˜ˆì‹œ ì±•í„°ëª… í• ë‹¹
                "content_en": chunk_en,
                "content_ko": chunk_ko,
                "corresponding_chunk_id": None # DB ì ì¬ ì‹œ ì±„ì›Œì§ˆ ì„ì‹œ ê°’
            })
            time.sleep(0.5) # API í˜¸ì¶œ ì œí•œ ë°©ì§€ ë° ì§„í–‰ ìƒí™© í™•ì¸ì„ ìœ„í•œ ë”œë ˆì´

        # 5. ìµœì¢… JSON êµ¬ì¡° ìƒì„±
        final_data_structure = [{
            "book_id": book_id,
            "gutenberg_id": book_meta['gutenberg_id'],
            "title_ko": book_meta['title_ko'],
            "title_en": book_meta['title_en'],
            "author": book_meta['author'],
            "chunks": processed_chunks
        }]
        
        # 6. JSON íŒŒì¼ ì €ì¥
        with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
            json.dump(final_data_structure, f, ensure_ascii=False, indent=4)
            
        print(f"\nì„±ê³µ: ìµœì¢… JSON íŒŒì¼ì´ '{OUTPUT_JSON_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    # ì‹¤ì œ LLM API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # dotenv ë“±ì„ ì‚¬ìš©í•˜ì—¬ ë¡œë“œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    main()